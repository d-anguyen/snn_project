I would suggest to run the jupyter notebook 'SNN_regions.ipynb' instead of 'main.py' as it's not updated. What the code does now is to choose a toydataset (linear,relu,dynamic or xor) and train a snn or ann on that. For the snn, we have the option to use either the spike outputs or the membrane potential outputs, currently I'm using the "spike count loss" which means that the actual output vector (which together with the one-hot label is the input of CE loss) consists of the spike count in each output neuron over time steps. Furthermore there are some help functions for plotting the results (visualization of the predicted/target labels and visualization of regions both before and after training). 

Unfortunately there is no option to somehow re-design the network architecture, so you will need to do that in 'models.py' by hand. Fixing this (and including other surrogate gradients) should be easy, but we need to think about which kind of architecture we want to apply. I guess we don't want pre-designed architectures like ResNet or AlexNet, but perhaps focus on simply MLP, say of size in-w1-w-...-w-out, and then vary the values of w1 (first hidden layer's width), w (subsequent layers' width) and d (number of subsequent layers). 

Everything now is very naively implemented, so suggestions (e.g. on which regularization we should use) would be very helpful!
